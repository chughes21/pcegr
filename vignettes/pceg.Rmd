---
title: "The pcegr Package"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{The pcegr Package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
library(kableExtra)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The pcegr package can be used to fit Poisson Chain Event Graphs (PCEGs) and Zero-Inflated Poisson Chain Event Graphs (ZIPCEGs) to data. We will demonstrate the functionality of the package using a simulated example based around knee pain. 

Suppose a study is carried out on the instances of knee pain in a population. For each individual, their age, weight and whether they have injured their legs before are recorded, along with the amount of instances of knee pain they suffered over a certain period of time, in years. This period of time is variable between individuals, and varies uniformly between 1 day and 2 years.

Each of the three covariates are binary, with categories and associated binary values:
+ Age (A): Old (0) or Young (1)
+ (Over)weight (W): Yes (0) or No (1)
+ History (H): Yes (0) or No (1)

while the response variable of Knee Pain (K) is assumed to come from a Zero-Inflated Poisson process where the risk probability and intensity is dependent on these three covariates. For each individual, they are assumed to have a latent risk state (S), either At Risk (0) or Risk Free (1), where those who are At Risk have a nonzero probability of suffering an instance of knee pain. 

For an individual with risk probability $\pi$ and Poisson intensity $\lambda$, the expected number of event counts $Y$ observed over time $t$ is $E[Y] = \pi \lambda t$. Assume instead that a Poisson process with rate $\mu$ was used to model the counts, with rate estimated as $\hat{\mu}$. We would expect (for a large sample size) that \[\hat{\mu} \simeq \pi \lambda.\] Now, if we assume for two populations with different covariates that having the same rate $\lambda$ means they have the same risk probability $\pi$, a significant simplification, then we can fit either a PCEG model or ZIPCEG model to the data set, and the estimated rates and risk probabilities should be in line with the relationship above.

As these models are based on staged trees, we will use leaf to refer to a specific covariate combination, and also the individuals with that specific covariate combination. We use the notation $l_i$ for the $i^{\text{th}}$ leaf in the tree, and assume it has rate $\lambda_i$ and associated risk probability $\pi_i$. A leaf stage $u$ contains leaves that are assumed to have the same rate. That is: $l_i, l_j \in u \Rightarrow \lambda_i = \lambda_j$.

Generally, we would not assume that $\lambda_i = \lambda_j \Rightarrow \pi_i = \pi$, but for the purposes of simulating the data in this package we have. This is a simplification, but means that when a PCEG is fit instead of a ZIPCEG, the merging for the leaves will be the same, but the PCEG will have lower posterior expected values, as outlined above. We have chosen this simplifying assumption in order to easily compare the PCEG and ZIPCEG models.  

We have thus simulated $N=10000$ observations for the knee pain data set from a zero-inflated Poisson distribution with uniformly distributed observation times. The data has been simulated under the following conditional independence statements:
+ Weight is independent of Age: W $\indep$ A;
+ History is independent of Weight given Age: H $\indep$ W $\vert$ A;
+ Knee Pain (and thus State) is independent of Age given History and the individual is Overweight: K, S $\indep$ A $\vert$ H, W = "Over"

In the following table, we can see each conditional transition probability, informed by the conditional independence statements above. Each probability has its definition alongside its associated stage and situations.
```{r}
library(pcegr)
cov_probs
```

For each leaf stage, we have the definition of the stage and its constituent leaves based on the conditional independence statements above, alongside the associated risk probability and rate.
```{r}
leaf_params
```

We can thus isolate the true values for $\pi$ and $\lambda$, along with what should be the PCEG estimate.
```{r}
true_lambda<-leaf_params$Rate
true_p<-leaf_params$Risk_Prob
pceg_lambda<-true_lambda*true_p;pceg_lambda
```

This simulation creates two data sets. The first, *knee_pain*, is the simulated data set, containing the true risk state for each individual. The second, *knee_pain_obs*, is the observed data set, which is simply *knee_pain* with the risk states removed. We can investigate these:
```{r}
head(knee_pain)
summary(knee_pain)
summary(knee_pain_obs)
```

First, we will fit a PCEG model to the data set. This requires specifying an effective sample size. The default choice for CEGs is usually the greatest number of categories a single variable takes, in this case 2. A sensitivity analysis can be carried out on the effects of this choice. 

As there are $2^3 = 8$ leaves, we will assume each has a prior weight of $a = \tfrac{2}{8} = 0.25$. In order to enforce a prior mean for the rate of 2.5, the simple average for the data set, we choose $b = \tfrac{0.25}{2.5}=0.1$. If we were to desire more bespoke priors for each leaf, they could be input as vectors rather than scalars.
```{r}

pmod<-pceg(knee_pain_obs,2,TRUE,TRUE, gamma_alpha = 0.25,gamma_beta = 0.1)
ptree<-staged.tree.creator(knee_pain_obs,pmod)
plot(ptree)
```

We can see that the model has correctly identified that the individuals of either Age who are Overweight with a History of injury are in the same stage ($s_{10}$ and $s_{14}$), and likewise for those with No History ($s_{9}$ and $s_{13}$). It has also correctly identified that History is independent of Weight given Age ($s_{3}$ and $s_{4}$ versus ($s_{5}$ and $s_{6}$)). 

We can also see this staged tree in pceg form:

```{r}
pceg1<-sceg(ptree)
plot(pceg1)
```


We now can estimate the posterior expected values of the transition probabilities for the covariates, and the rates for the response. The transition probabilities can be compared to *cov_probs*, while we have included what the PCEG estimate should be for $\lambda$, based on the true $\pi$ and $\lambda$.
```{r}
for(i in 0:2){
  print(colnames(knee_pain_obs)[i+1])
  print(value_extractor(knee_pain_obs,pmod,level_rel_final = i-3,zip=FALSE,dec_place=2))
}
value_extractor(knee_pain_obs,pmod,level_rel_final = 0,true_value = pceg_lambda, zip=FALSE,dec_place=2)
```
As can be seen, while the model is misspecified, it correctly identifies the transition probabilities, and the estimates of the rates are close to what we would expect.

We can also find 95% highest density intervals for these parameters. For example:
```{r}
hdi_gamma_extractor(knee_pain_obs,pmod,zip=FALSE)
hdi_beta_extractor(knee_pain_obs,pmod,level_rel_final = -1, zip=FALSE)
```

We can also check the log marginal likelihood and Chi-square calculation for the model, to assess the fit. 

```{r}
pmod$lik
expected_count_calculator(knee_pain_obs,pmod,zip=FALSE,dec_place=2)
```
In the Chi-square contribution matrix, we can see some large values. In particular, for the leaves with low rates, for example the first row, we can see large contributions for the higher event counts. This is because there are a much higher number of observed counts greater than 3 than we would expect based on the small rate. In contrast, for the leaves with higher rates, such as 4, 6 and 8, we can see that the contributions for the zero counts are much higher. This is because there are much more observed zero counts than would be expected with such a high rate. 

These results are all indicative of the true underlying model instead being a zero-inflated Poisson distribution. We thus fit a ZIPCEG model to the data. 

To fit a ZIPCEG model, we must first choose the method of parameter estimation. As time is variable, we cannot use the MLE or method of moments estimators. Thus, the choices are Gibbs sampling ("Gibbs"), numerical likelihood optimisation ("nlm") or the Expectation-Maximisation algorithm ("EM"). We will perform model selection using the first two choices.

We will keep the same effective sample size as for the PCEG, but we must consider the prior on the leaves. For a ZIPCEG, we want the risk free leaves to have zero prior hyperparameters, and the risk leaves will have the same hyperparameters as for the PCEG. The priors must be input as vectors, where every odd element is a risk free leaf and thus 0. There are 16 leaves in total, 8 of each type.
```{r}
a<-rep(c(0,0.25),8)
b<-a/2.5
```

For the Gibbs sampler, we must also specify the hyperparameters of the Beta($c,d$) prior for the risk probability. We will assume each risk probability has the same prior. This prior should be cohesive with the prior in the tree itself, and thus we should have $c = d = \tfrac{2}{16} = 0.125$ for each risk probability.
```{r}
c<-0.125
d<-0.125
```
We will also run the Gibbs sampler for $N=10000$ iterations. When the parameeters are estimated using the Gibbs sampler, we will impute the states stochastically.
```{r}
zipmod1<-zipceg(knee_pain_obs,"Gibbs",iter=10000,variable_time = TRUE, stoch_imputation = TRUE, gamma_alpha = a, gamma_beta = b, beta_c = c, beta_d = d)
ziptree<-staged.tree.creator(knee_pain_obs,zipmod1)
zipceg1<-sceg(ziptree)
plot(ziptree)
plot(zipceg1)
```
We can see that the model correctly merges the risk states (8 and 9)and the rates (15 and 16). We can also see that all of the risk free leaves are merged together into one stage (12). The covariate relationships have also been preserved from the PCEG. 

We now compare the goodness of fit to the PCEG. First, we calculate the log marginal likelihood and the Chi-square calculation. 
```{r}
zipmod1$lik
expected_count_calculator(knee_pain_obs,zipmod1,zip=TRUE,dec_place=2)
```
We can see that the Chi-square value is significantly lower for the ZIPCEG model, which is to be expected as the model was simulated assuming a zero-inflated Poisson distribution.

As for the log marginal likelihood, this is not directly comparable to the PCEG model, due to the extra covariate and different priors. The key here is to fit a ZIPCEG model to the data with the same prior as the regular ZIPCEG model, except every individual is assumed to be at risk. This can be done with the *pceg* function as follows:
```{r}
data.risk<-state_imputer(knee_pain_obs,all_risk=TRUE)
pmod.bf<-pceg(data.risk,2,TRUE,TRUE,TRUE,gamma_alpha=a,gamma_beta=b)
pmod.bf$lik
```
This is the exact same model as the regular PCEG, except we can see that the log marginal likelihood has decreased. This log marginal likelihood is comparable to the ZIPCEG.
```{r}
zipmod1$lik-pmod.bf$lik
```
Hence, the ZIPCEG is a significantly better fit to the data than the PCEG according to the log Bayes factor also. 

We can also fit the ZIPCEG using the *nlm* method instead. Notably, this doesn't require a Beta($c,d$) prior, or a number of iterations. It is also significantly faster.  
```{r}
zipmod2<-zipceg(knee_pain_obs,"nlm",variable_time = TRUE, stoch_imputation = TRUE, gamma_alpha = a, gamma_beta = b)
zipmod2$result
zipmod2$lik
expected_count_calculator(knee_pain_obs,zipmod2)
```
Comparing the two ZIPCEG models fitted appears to favour the Gibbs sampler method.
```{r}
zipmod1$lik-zipmod2$lik
expected_count_calculator(knee_pain_obs,zipmod1)$chi_sq-expected_count_calculator(knee_pain_obs,zipmod2)$chi_sq
```
However, both of these models are the result of one iteration, and the ZIPCEG model selection process is not deterministic. We can thus carry out the model selection a number of times, for example 100, and compare the risk probabilities and rates for each leaf, as well as select the MAP model from those iterations. Comparing the probabilities and rates for each leaf gives an idea of how the leaves merge with each other, as those who have similar plots will merge regularly. This is useful for data that is not simulated, as there may be significant variability from iteration to iteration. 

we will use violin plots to visualise, but there are options to use histograms, scatter plots, or line plots. We will use the *nlm* method, due to its lower computational cost per iteration. 
```{r}
mapmod<-zipceg.iter(knee_pain_obs,"nlm",iter_total=100,variable_time = TRUE,gamma_alpha=a,gamma_beta=b)$mod
mapmod.alt<-zipceg.iter(knee_pain_obs,"nlm",iter_total=100,variable_time = TRUE,plot_rates=FALSE,plot_probs=TRUE,gamma_alpha=a,gamma_beta=b)
```
We can see in the plot of the rates the merging that is present for leaves 3 and 7, and 4 and 8. The violin plots are identical, indicating that they always merge, and so we can be confident in the result from our initial model selection. In general, the violin plots are quite tight, which is a result of the large sample size and demarcation between the leaves. The risk probabilities also show these mergings, although not as consistently as for the rates. For example, leaf 4 occasionally merges with leaf 6 instead of 8. In both plots, leaf 5 is by far the most variable, and this is a result of it having the smallest sample size.

After looking at the plots above, we can be confident in the mergings produced by the single iteration of the model selection process. However, the way the states are imputed can still significantly affect the log marginal likelihood of the final model. We have selected a MAP model, and we can investigate its properties.
```{r}
mapmod$result
mapmod$lik
mapmod$lik-zipmod1$lik
expected_count_calculator(knee_pain_obs,mapmod,zip=TRUE)$chi_sq-expected_count_calculator(knee_pain_obs,zipmod1,zip=TRUE)$chi_sq

```
Interestingly, the MAP model outperforms the single iteration of the Gibbs sampler in terms of log marginal likelihood, but not in terms of the Chi-square. We can compare the matrix of their Chi-square contributions to examine why.
```{r}
expected_count_calculator(knee_pain_obs,mapmod,zip=TRUE)$chi.mat-expected_count_calculator(knee_pain_obs,zipmod1,zip=TRUE)$chi.mat
```
We can see that there are some differences between the two models for leaves 1, 3 and 6. We now look at the posterior expected values of $\pi_i$ and $\lambda_i$ from the MAP model. We also compare these to the true values of $\lambda_i$ and $\pi_i$. Note that the estimates below are for each stage, not leaf. For example, stage 3 contains leaves 3 and 7. The first row for $\lambda$ is the risk free stage, and so the second row is stage 1.  
```{r}
value_extractor(knee_pain_obs,mapmod,true_value=true_lambda)
value_extractor(knee_pain_obs,mapmod,level_rel_final = -1,true_value=true_p)

```
We can also calculate the expected values for the single iteration of the Gibbs sampler.
```{r}
value_extractor(knee_pain_obs,zipmod1,true_value=true_lambda)
value_extractor(knee_pain_obs,zipmod1,level_rel_final = -1,true_value=true_p)
```
Rather unexpectedly, the *zipmod1* appears to generally outperform *mapmod* in estimating the true values of the parameters. The key to why this happens lies in the imputation of states. 

We will once again fit a PCEG model, except this time to the true data. 
```{r}
truemod<-pceg(knee_pain,2,TRUE,TRUE,gamma_alpha=a,gamma_beta=b,zip=TRUE)
truemod$lik
truemod$result
value_extractor(knee_pain,truemod,true_value=c(0,true_lambda),zip=FALSE)
value_extractor(knee_pain,truemod,level_rel_final=-1,true_value=true_p,zip=FALSE)
```

We can also use the *pceg* function to carry out variable discretisation. To demonstrate this, we will use two new data sets. The first, *knee_pain_age* is the same data set as *knee_pain*, except the Age covariate is now continuous. Any individual who was Young previously now has an integer value uniformly distributed between 1 and 40, while an Old individual has an integer between 41 and 80. The *knee_pain_age_obs* data set is the observed *knee_pain_age* data set, and so has the State covariate removed, as before. 

The *pceg* function has several inputs related to variable discretisation. They are:
+var_disc: The variable in the order of which to discretise. Generally, we will want the variable of interest to be the last covariate and immediately precede the response.
+disc_length: The length over which to search for stages to merge. For example, if this is 1, it will only merge stages that are immediate neighbours when ordered. This is particularly useful when dealing with sparse edge counts, as it enables the user to adjust their search length where necessary.  
+restrict: Whether or not stages can only be merged when they have the same covariate values for everything else. This will generally be assumed to be true, as the goal is to discretise the variable, which requires the rest of the process to be kept fixed for each possible evolution.
+mirror: If TRUE, this will have the same discretisation for each set of covariates. If the goal is to find one single discretisation (i.e. anything over 40 is old), this should be TRUE. However, if this is FALSE, it allows the flexibility for each unfolding of the process to have its own discretisation (i.e. 40 could be old for men, but still young for women.)
+cat_limit: The minimum number of categories the variable can be discretised to. 

We will demonstrate the variable discretisation functionality of the *pceg* function in two ways. First, we will fit a Vanilla CEG, then we will fit a PCEG model, in both cases discretising the Age variable. In order to fit a Vanilla CEG, we must first create a suitable data set where the response variable is binary. The response variable we will choose is simply whether an event occured or not. Furthermore, any data set we will analyse is required to have Age as the last covariate, so it must be reordered. Finally, while the Age values in the data set are integers, they must be converted to factors and assumed as categorical in order for the functions to work.

```{r}
df<-knee_pain_age_obs[,c(2,3,1,4,5)]
ind<-which(df$y > 0)
resp<-factor(rep("No",length(df[,1])),levels=c("Yes","No"))
resp[ind]<-"Yes"
df$age<-factor(df$age,levels=c(min(df$age):max(df$age)))
df.bin<-data.frame(df[1:3],Pain = resp)
summary(df)
summary(df.bin)
```
We have now created two data sets: *df* for the PCEG and *df.bin* for the Vanilla CEG. We start with the Vanilla CEG. 

We first begin by performing the variable discretisation without any of the inputs described above. We also note that for the purposes of the equivalent sample size, Age is not assumed to have any categories and thus it is still 2.
```{r}
disc.mod0<-pceg(df.bin,2,var_disc=3)
disc.mod0$result
```
We can see that for the Normal Weight, No History and Overweight, Yes History groups, they have almost correctly identified the Age break-point of 40. However, because there was no restriction on interval length or discretisation between covariate combinations, the rest of the mergings are together and the result is rather nonsensical. 

We thus choose to restrict between combinations now. We also shorten the interval length to just look at neighbours. 

```{r}
disc.mod1<-pceg(df.bin,2,var_disc=3,disc_length=1,restrict=TRUE)
disc.mod1$result
```
We can see that the short interval length tends to lead to small stages. We thus increase the interval length to 2. 

```{r}
disc.mod3<-pceg(df.bin,2,var_disc=3,disc_length=2,restrict=TRUE)
disc.mod3$result
```
We can see that increasing the interval length has lead to both of the Overweight covariate combinations having only one category. If we instead require a minimum of 2 categories:
```{r}
disc.mod4<-pceg(df.bin,2,var_disc=3,disc_length=2,restrict=TRUE,cat_limit=2)
disc.mod4$result
```
We can see that now Over, Yes has 3 categories, rather than the expected 2. This is because the model is unable to join the 1-14 stage and 17-80 stage as it would like, as that would lead to just one stage. Meanwhile, the 15-16 stage leads to a decrease in the Bayes Factor so will not merge. Currently, there is no functionality to add a maximum number of categories, which would mean decreasing the Bayes Factor by the minimum amount until that number of categories is enforced, but this is something to consider for future updates to the function. 

We now instead mirror the mergings across each covariate combination. 

```{r}
disc.mod5<-pceg(df.bin,2,var_disc=3,disc_length=2,restrict=TRUE,mirror=TRUE,cat_limit=2)
disc.mod5$result
```
We can see that once the mergings are equivalent across all four covariate combinations, the model almost exactly names the correct discretisation. Considering the fact that this model is misspecified in the first place, this is an excellent result. 

We now see if this discretisation works as well when the model is a PCEG instead. We will begin by restricting to covariate combinations while also searching over an interval of 2.

```{r}
disc.pmod0<-pceg(df,2,poisson_response = TRUE, variable_time=TRUE,gamma_alpha=0.25,gamma_beta=0.1,var_disc=3,disc_length=2,restrict=TRUE)
disc.pmod0$result
```
We can see that the mergings are not particularly helpful. We now once again mirror the results across covariate combinations.

```{r}
disc.pmod1<-pceg(df,2,poisson_response = TRUE, variable_time=TRUE,gamma_alpha=0.25,gamma_beta=0.1,var_disc=3,disc_length=2,restrict=TRUE,mirror=TRUE)
disc.pmod1$result
```
We can see now that the model correctly identifies the Age breakpoint of 40 years of age, without even needing to specify a minimum number of categories. 




